---
title: AI weekly (17/2020)
subtitle: My selection of news on AI/ML and Data Science
date: 2020-04-25
bigimg: [{src: "/img/2020-04-25-ai-weekly-17-2020.jpg", desc: "Cherry blossom (Berlin 2020)"}]
tags: ["ai-news", "NLP"]
---


+++ The Surge of Sensationalist COVID-19 AI Research +++ Model-Based Machine Learning +++ 'Super Duper NLP Repo' and 'Big Bad NLP Database' +++


 
<!--more-->



## Breakthrough &mdash; Or So They Say

**The Surge of Sensationalist COVID-19 AI Research.** In this [article](https://www.news-medical.net/health/The-Surge-of-Sensationalist-COVID-19-AI-Research.aspx), Professor Hamid R. Tizhoosh, who leads University of Waterloo's KIMIA Lab (Laboratory for Knowledge Inference in Medical Image Analysis), is bashing the flood of hastily published articles on questionable "AI solutions" for COVID-19. Medical imaging is a highly sensitive domain in which a long process is generally required to curate and access a set of labeled images. Needless to say, the curation has to happen within the walls of a hospital not just because the experts are there, but also due to the required de-identification of images to comply with privacy regulations. Tizhoosh highlights that more and more collections of x-ray and partly CT images – scraped from the Internet – are appearing and evolving as the creators continue to add images. Because of the availability of such datasets on one side and the ubiquity of basic AI knowledge and tools on the other side, many AI enthusiasts and start-ups have impulsively begun to develop solutions for COVID-19 in x-ray images. Papers based on these data sets report no validation, and no radiologist has guided the experiments. Many of these works were hurriedly made public before the creators of datasets could even provide sufficient explanation about their collection process. In an attempt to overcome the small data size, AI enthusiasts and start-ups mix the few COVID-19 images with other public datasets, i.e., pneumonia datasets. Tizhoosh  urges that the AI community waits for real data from hospitals, does the ethics clearance and de-identification, and works with radiologists to develop solutions for chest issues of the future.
 


## Tools and Frameworks


 



## Business News and Applications





## Publications
 
**Model-Based Machine Learning.** Several decades of research in the field of machine learning have resulted in a multitude of different algorithms for solving a broad range of problems. To tackle a new application, a researcher typically tries to map their problem onto one of these existing methods, often influenced by their familiarity with specific algorithms and by the availability of corresponding software implementations. Christopher M. Bishop, researcher at Microsoft and author of a [leading textbook on machine leaning](https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/), describes an alternative methodology, in which a bespoke solution is formulated for each new application, in this [article](https://royalsocietypublishing.org/doi/10.1098/rsta.2012.0222) and a [book draft](http://www.mbmlbook.com/toc.html) (also as [PDF](http://www.mbmlbook.com/MBMLbook.pdf)). The solution is expressed through a compact modelling language, and the corresponding custom machine learning code is then generated automatically. This framework emerged from a  convergence of three key ideas: (1) the adoption of a Bayesian viewpoint, (2) the use of probabilistic graphical models, and (3) the application of fast, deterministic, efficient and approximate inference algorithms. The core idea is that all assumptions about the problem domain are made explicit in the form of a model, which is here understood to be a set of assumptions about the world expressed in a probabilistic graphical format with all the parameters and variables expressed as random components.



## Tutorials

**'Super Duper NLP Repo' and 'Big Bad NLP Database'.** There are 2 major components of a machine learning modeling project of any kind: the data, and the algorithms which learns something from this data. The folks at Quantum Stat, a NY based NLP / AI startup, have recently released two stepping stones that can help anyone interested in NLP pick and choose the right components for his interests and tasks. (1) The [Big Bad NLP Database](https://datasets.quantumstat.com/) is a collection of nearly 300 well-organized NLP datasets, ready to go for common NLP tasks and needs, such as document classification, question answering, automated image captioning, dialog, clustering, intent classification, language modeling, machine translation, text corpora, and more. (2) The [Super Duper NLP Repo](https://notebooks.quantumstat.com/), a collection of Colab notebooks covering a wide array of NLP task implementations available to launch in Google Colab with a single click. Notebook entries in the repo include a general description, the notebook's creator, as well as the task (text classification, text generation, question answering) being considered and the type of model being employed (BERT, GPT-2, convolution neural network, etc.). These notebooks come from across the web, including from expert individuals and organizations.

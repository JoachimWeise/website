---
title: AI weekly (49/2019)
subtitle: My selection of news on AI/ML and Data Science
date: 2019-12-07
# lastmod: 2019-11-29
bigimg: [{src: "/img/2019-12-07-AI-weekly-49-2019.jpg", desc: "Wiener Staatsoper (2019)"}]
tags: ["ai-news", "alibaba", "aws", "facebook", "reinforcement-learning"]
---

+++ AWS launches SageMaker Studio, a web-based IDE for machine learning +++ Alibaba: Dynamic Pricing with Deep RL +++ 


<!--more-->

## Breakthrough &mdash; Or So They Say




## Tools and Frameworks

**AWS launches SageMaker Studio, a web-based IDE for machine learning.** AWS has [announced the launch](https://aws.amazon.com/de/blogs/aws/amazon-sagemaker-studio-the-first-fully-integrated-development-environment-for-machine-learning/) of SageMaker Studio, a web-based IDE for building and training machine learning workflows. It includes everything a data scientist would need to get started, including ways to organize notebooks, data sets, code and models, for example. It essentially wants to be a one-stop shop for all the machine learning tools and results you need to get started. At the core of Studio is also the ability to share projects and folders with others who are working on the same project, including the ability to discuss notebooks and results. SageMaker Studio is integrated with AWS’s SageMaker machine learning service, which can automatically scale based on your needs. It now includes a debugger, a monitoring tool and Autopilot, which automatically creates the best models for you based on your data, with full visibility into how it decides to build your models. Related to this, AWS also launched SageMaker Notebooks, also integrated into Studio, as a managed service. Data scientists won’t have to provision instances for this as they will automatically provision them as necessary.




## Business News and Applications




## Publications

**Dynamic Pricing On E-commerce Platform With Deep Reinforcement Learning** [(arXiv:1912.02572v1)](https://arxiv.org/abs/1912.02572). A team from Alibaba Supply Chain Platform (Hangzhou) outlines a framework that represents dynamic pricing as a Markov Decision Process (MDP). The agent periodically changes product prices as its action after observing the environment state. Each pricing episode ends when the product is out of stock. The model is pre-trained based on historical sales data and previous specialists’ pricing actions. The Alibaba team uses 'difference of revenue conversion rates' as a reward function which is supposed to indicate if pricing actions had a positive impact on revenue conversion. The framework was tested offline and online and showed better performance than manual pricing by operations experts.


**Training multi-agent AI systems to solve complex tasks through cooperation** [(arXiv:1910.08809).](https://arxiv.org/abs/1910.08809) A novel approach to cooperative multi-agent reinforcement learning that assigns tasks to individual agents within a group, thereby improving the entire group’s ability to collaborate, is published by [facebook AI](https://ai.facebook.com/blog/using-multi-agent-reinforcement-learning-to-improve-collaboration/) with [source code on github](https://github.com/TorchCraft/TorchCraftAI/tree/targeting). The approach focuses on multi-agent collaborative (MAC) problems where agents have to carry out multiple intermediate tasks in order to accomplish a larger one. As the number of agents and tasks in these kinds of MAC problems increases, the complexity grows exponentially, which prevents systems from learning directly from large-scale scenarios. 
Systems must instead generalize from smaller scenarios and tackle tasks that were not part of their RL-based trial-and-error training runs. Since RL-trained systems often struggle with this exact type of generalization, the approach breaks MAC policies down into high- and low-level policies. The high-level policies determine which agents should be assigned specific tasks. To encourage collaboration between agents, a quadratic cost function is employed that optimizes for long-term performance.


## Tutorials


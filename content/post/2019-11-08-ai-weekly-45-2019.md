---
title: AI weekly (45/2019)
subtitle: My selection of news on AI/ML and Data Science
date: 2019-11-08
# lastmod: 2019-11-29
bigimg: [{src: "/img/2019-11-08-ai-weekly-45-2019.jpg", desc: "Nice (2019)"}]
tags: ["ai-news"]
---

+++ How to train artificial intelligence that won’t destroy the environment +++ Deep learning has a size problem +++ Quantifying the Carbon Emissions of Machine Learning +++


<!--more-->

## Breakthrough &mdash; Or So They Say

 


## Tools and Frameworks

  


## Business News and Applications
 



## Publications

**How to train artificial intelligence that won’t destroy the environment**. The carbon footprint of machine learning is bigger than you think. An article by [The Outline](https://theoutline.com/post/8186/artificial-intelligence-destroy-environment?zd=1&zi=4ljijbfe) features recent publications on the environmental consequences of deep learning. A study [(arXiv:1906.02243)](https://arxiv.org/abs/1906.02243) by researchers  at the University of Massachusetts Amherst had found that training just one AI model produces as much carbon dioxide equivalent as nearly the lifetime emission of five average American cars. In a similar vein, a preprint [(arXiv:1907.10597)](https://arxiv.org/abs/1907.10597) by researchers from the Allen Institute for AI, Carnegie Mellon University and the University of Washington, partly based on the [earlier blog post "AI and Compute"](https://openai.com/blog/ai-and-compute/), reports that the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.4-month doubling time (by comparison, Moore’s Law had a 2-year doubling period). Since 2012, this metric has grown by more than 300,000x (a 2-year doubling period would yield only a 7x increase). Potential remedies are discussed, e.g. sharing pre-trained models and transparent benefit-to-cost assessments of potential model runs (see articles below).


**Deep learning has a size problem.** A related, [very good post](https://heartbeat.fritz.ai/deep-learning-has-a-size-problem-ea601304cd8) on model efficiency. It goes deep in certain areas but remains extremely accessible at all points. In the long term, the resource hunger of modern deep learning models is going to cause a few problems. Quote: "First, it hinders democratization. If we believe in a world where millions of engineers are going to use deep learning to make every application and device better, we won’t get there with massive models that take large amounts of time and money to train. Second, it restricts scale. There are probably less than 100 million processors in every public and private cloud in the world. But there are already 3 billion mobile phones, 12 billion IoT devices, and 150 billion micro-controllers out there. In the long term, it’s these small, low power devices that will consume the most deep learning, and massive models simply won’t be an option."


**Quantifying the Carbon Emissions of Machine Learning.** Researchers from Element AI, Université de Montréal and Polytechnique Montréal just published a preprint [(arXiv:1910.09700)](https://arxiv.org/abs/1910.09700) on their [Machine Learning Emissions Calculator](https://mlco2.github.io/impact/). Quote: "From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions."



## Tutorials
 

